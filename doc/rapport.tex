%\documentclass[journal, a4paper]{IEEEtran}
\documentclass[utf8,9pt]{extarticle}
\usepackage[T1]{fontenc}       % Encodage le plus étendu
\usepackage[utf8]{inputenc}    % Source Unicode en UTF-8
\usepackage{capt-of}  % <---
\usepackage{cuted}    % <===
\usepackage{listings}
\usepackage{mdframed}
\usepackage{multirow}
\usepackage[cyr]{aeguill}
\usepackage{geometry}
\usepackage{float}
\usepackage{biblatex}

\addbibresource{sample.bib}

% Define colors
\definecolor{background}{rgb}{0.95,0.95,1}
\definecolor{keywords}{rgb}{0,0,0.6}
\definecolor{comments}{rgb}{0.25,0.5,0.35}
\definecolor{strings}{rgb}{0.6,0,0}

% Configure listings
\lstset{
    backgroundcolor=\color{background},
    basicstyle=\ttfamily\footnotesize,
    keywordstyle=\color{keywords}\bfseries,
    commentstyle=\color{comments}\itshape,
    stringstyle=\color{strings},
    numbers=left,
    numberstyle=\tiny,
    frame=single,
    breaklines=true,
    breakindent=20pt,
    prebreak=\textbackslash,
    showstringspaces=false,
    captionpos=b,
    language=Python
}

\usepackage[
  separate-uncertainty = true,
  multi-part-units = repeat
]{siunitx}
\usepackage{amsmath}
\usepackage{mathtools}% http://ctan.org/pkg/mathtools
\usepackage{hyperref}
\usepackage{float}
\usepackage{graphicx} 
\usepackage{url}    
\usepackage{stfloats} 
\usepackage{amsmath}   

\usepackage{lipsum} % Dummy text


% En-tête et pied de page
\usepackage{lastpage}
\usepackage{placeins}
\usepackage{tabularx}
\usepackage{etoolbox} % for \BeforeBeginEnvironment and \AfterEndEnvironment

\BeforeBeginEnvironment{tabular}{\vspace{0.5cm}} % Add space before the table
\AfterEndEnvironment{tabular}{\vspace{0.5cm}}
\renewcommand{\sectionmark}[1]{\markright{#1}}



\geometry{bottom=0.75 in}
% Your document starts here!
\begin{document}
\begin{titlepage}

\newcommand{\HRule}{\rule{\linewidth}{0.5mm}}   	% horizontal line and its thickness
\center 
 
% University
\textsc{\LARGE Université Libre de Bruxelles}\\[1cm]

% Document info
\textsc{\Large }\\[0.2cm]
\textsc{\large INFO-F-415}\\[1cm] 


\HRule \\[0.8cm]
{ \huge \bfseries Project report: Advanced Databases}\\[0.3cm]
{ \huge \bfseries}
{ \huge \bfseries\textit{Stream Databases – Apache Kafka \& Apache Flink}}\\[0.7cm]
\HRule \\[0.8cm]
\large
\vspace{4cm}
\includegraphics[scale=0.2]{img/logo.png}
\vfill
\emph{Authors:}\\
Grégoire Jean-Nicolas : 446638 (M-INFO)\\ 
Installé Arthur : 495303 (M-INFO) \\
Vanderslagmolen Moïra : 547486 (M-INFO) \\
Ze-xuan Xu : 541818 (M-INFO)\\
\vspace{1cm}
\emph{Date:} December 2024\\
\vfill
\end{titlepage}

\newpage
\tableofcontents
\newpage


\section{Introduction}
\indent \hspace{2mm} The project we chose is stream databases with two different tools: Apache Flink and Apache Kafka. In this report, we present a short introduction to the stream databases. So we present Apache Flink and Apache Kafka and how stream databases are implemented in these tools. Finally, we show how we implemented the same application with three different tools : 
\begin{itemize}
    \item Apache Kafka
    \item Apache Flink
    \item Apache Kafka and Apache Flink working together
\end{itemize}
and we will compare these three tools using benchmarking.


\section{Stream Databases}
\indent \hspace{2mm} Stream databases are specialized to handle real-time data streams and huge amount of continuous query. It is designed to process, store and analyze real-time data streams unlike traditional databases which work on static data batches. We use stream databases in a large number of sectors, like chat, "Internet Of Things" (IoT), fraud detection in financial transactions, transport logistics, monitoring, etc.

\subsection{How does stream databases work?}

\indent \hspace{2mm} To understand how stream databases work, it is important to know the difference between stream (used by stream databases) and batch processing (used by traditional databases).
These are two ways to handle data which work very differently.

\begin{figure}[H]
    \centering
    \includegraphics[scale=0.7]{img/batchvsstream.png}
    \caption{Comparison between stream databases and batch processing}
    \label{fig:stream-batch}
\end{figure}

\indent \hspace{2mm} As we can see in Figure \ref{fig:stream-batch}, batch processing process the data by batches, which means that the processor waits until it has enough data to send it.
Stream databases do the opposite and sends data directly.When data streams enter inside a stream database, it is immediately processed and try to get the next one.

\section{Comparison}
\subsection{Traditional databases}
\subsubsection{Properties}
\begin{itemize}
    \item Centralized storage:
        \begin{itemize}
        \item Data are stored in tables with fixed schema.
        \item Optimized for structured data with strong consistency
        \end{itemize}
    \item Batch processing:
        \begin{itemize}
        \item Data are processed in bulk
        \item Not designed for real-time or continuous data flow.
        \end{itemize}
    \item Transaction Management:
        \begin{itemize}
        \item Strong ACID (Atomicity, Consistency, Isolation, Durability) guarantees.
        \item Suitable for applications like banking, e-commerce, and CRM.
        \end{itemize}
    \item Query Optimization:
        \begin{itemize}
        \item SQL is the dominant query language.
        \item Indexing and normalization used for efficient query performance.
        \end{itemize}
\end{itemize}

\subsubsection{Strengths \& Limitations}
\hspace{2mm}  Since traditional databases have been used for storing and querying data for decades, they are reliable and mature. Systems such as PostgreSQL, MySQL, and Oracle Database ensure data's consistency, durability and reliability. They are a natural choice for many applications because they established an ecosystem with rich tooling and community support.

One of the primary strengths of traditional databases is their suitability for predictable workloads, where the data and access patterns are fixed. These systems can handle complex queries and transactions easily, offering developers advanced capabilities such as indexing, joins, and stored procedures. These features make traditional databases ideal for e-commerce, inventory and content management.

However, traditional databases have some limitation. For example, high latency can be an issue when scaling systems working with larger or more distributed workloads. Additionally, traditional databases may struggle to scale, particularly when the volume or velocity of data surpasses their capacity.

Finally, they can also be limited by their schema rigidity when adapting to unstructured or semi-structured data, such as JSON or XML formats.

\subsection{Stream database}
\subsubsection{Property}
\begin{itemize}
    \item Stateful Stream Processing:
        \begin{itemize}
        \item Aggregations (sum, count, average, ...).
        \item Statefull operation 
        \end{itemize}
    \item Continuous Query Processing:
        \begin{itemize}
        \item Results are updated based on changes in data streams rather than recomputing everything from scratch
        \item keep getting query and processing them one by one.
        \end{itemize}
    \item  Time-Ordered Processing:
        \begin{itemize}
        \item Events are processed in temporal order
        \item Support handling late-arriving or out-of-order events
        \end{itemize}
    \item  Low Latency:
        \begin{itemize}
        \item Designed to minimize the delay between data ingestion and query result generation
        \end{itemize}
    \item Event-Driven architecture:
        \begin{itemize}
        \item Each event triggers computations
        \item Enabling real-time updates to alerts, updates, ...
        \end{itemize}
\end{itemize}

\subsubsection{Strengths \& Limitations}
\hspace{2mm} Stream databases are designed for real-time analytics and event-driven architectures. They are suitable for scenarios where data arrives continuously and decisions need to be made almost instantaneously. Systems like Apache Kafka Streams, Apache Flink, and Amazon Kinesis let you process data quickly and support continuous querying. This is useful in industries like financial trading, IoT, and real-time monitoring systems. In other words, they can process data as it comes in, offering low latency and real-time analytics since they can react to changes or events as they occurs. This allows them to receive updates without manual intervention. 

Stream databases have some limitations. They often lack support for complex operations, such as multi-step joins or aggregations across large datasets. It can also be difficult to set up stream databases to process data given that you have to plan carefully for things like adding historical data or reapplying transformations.

Another aspect of stream databases that can be limiting is that it's monitoring and debugging can be complex. This is due to the fact of the continuous nature of data flow and real-time processing introduces additional layers of complexity compared to batch-based systems.


\section{Tools}
\subsection{Apache Kafka}
\hspace{2mm} Apache Kafka is a platform for streaming data in real time. It is widely used for building systems that process data streams efficiently and reliably. 
Kafka operates a cluster of servers, with key components:
\begin{enumerate}
    \item \textit{Brokers}: Servers that store event streams and manage data replication. These events are stored and distributed across partitions within topics for scalability.
    \item \textit{Producers}: Applications or systems that send events to Kafka topics.
    \item \textit{Consumers}: Applications that read and process events. Kafka lets consumers process data without affecting producers.
    \item \textit{Partitions}: Topics are split into parts, allowing multiple reads and writes. Events with the same key are stored together in the same partition. It also allows to replicate the data if there is a crash of one partition.
    \item \textit{Replication}: Data in partitions is replicated across brokers to ensure reliability and availability even if servers fail.
\end{enumerate}

We can see Apache Kafka as a processing engine which uses stream processing. It uses a TCP-Network protocol to communicate between the server (\textit{Brokers}) and clients (\textit{Producers}). Those clients publish (\textit{write}) events and consumers are those that subscribe to (\textit{read \& process}) these events. In Kafka, producers and consumers are fully decoupled and agnostic of each other. For example, producers never need to wait for consumers.



\begin{figure}[h!]
    \centering
    \includegraphics[scale=0.25]{img/kafka_partitioning.png}
    \caption{This example topic has four partitions $P_1 - P_4$. Two different producer clients are publishing, independently from each other, new events to the topic by writing events over the network to the topic's partitions. Events with the same key (denoted by their color in the figure) are written to the same partition. Note that both producers can write to the same partition if appropriate. \cite{kafka}}
    \label{kafka_partitioning}
\end{figure}

It is also important to note the key roles of Apache Zookeeper in Apache Kafka. Zookeeper is a centralized service for maintaining configuration information, naming, providing distributed synchronization, and providing group services \cite{zookeeper}. In Apache Kafka, Zookeeper keep tracks of the configurations of brokers and topics. It also elects the leader of the partitions, which is responsible for handling read and write operations on the partitions.


\subsection{Apache Flink}

\hspace{2 mm} Apache Flink is a framework and distributed processing engine. It uses stream processing but can also be used to do batch processing. In Flink, a data stream represent a collection of data. Data Streams are created from various sources such as files, databases, http server, socket streams, etc. The data imported from the source can be finite or unbounded. A Flink job is a job that will read from the data source and process it immediately and send it to the sink.
A Flink cluster is a cluster of jobs.


\subsection{Apache Kafka and Apache Flink}

\hspace{2 mm} Apache Kafka and Flink are very often used together. The reason behind this cooperation is that Flink provides the computation and the consistency while Kafka provides the durability with the topics and the brokers.





\section{Implementation}
\hspace{2 mm} The application we implemented is a live chat that moderates automatically. Each message is processed and analyzed when it pass through the stream database. If the message contains "illegal" words, we output that the message contains illegal words and is therefore not published. We chose a live chat because each message needs to be computed very fast and there can be huge amounts of messages. We want to mimic the constant flux that are send to other users in a server-based architecture such as Twitch. 

The language we chose for this application is Java and Python. We chose Java for our consumers because Flink is implemented in Java and Scala and has the best documentation for Java code. Each consumers (Kafka, Flink and Kafka-Flink) is implemented in Java for consistency for the benchmark. Each producers is implemented in python because Kafka has a great library for Python called kafka-python-ng and for simplicity. 


\subsection{Data}

\hspace{2 mm} The data we used comes from \href{https://www.kaggle.com/datasets/kazanova/sentiment140}{this dataset}. It contains 1.6 millions tweets extracted from the Twitter API with the following fields : the target (a scale from 0 to 4 representing sentiment, 0 being negative), the id, the date, the flag, the user, and the message content. The format of the data is a csv file.

\subsection{Apache Kafka}

\hspace{2 mm} The first tool we implemented is Apache Kafka. For Apache Kafka, we need a topic, a producer (which will send the messages), a consumer (which will receive the messages), and a sink (the output of the messages).

\subsubsection{Producer}

\hspace{2 mm} We implemented the producer of Kafka in Python. We initialize a Kafka Producer that connects to a port and serializes with json.
Then, we iterate through a file and for each line in the file we send with the producer to the topic chat the message in format json. The Kafka topic temporarily stores the message and waits until a consumer connects to the topic. Each consumer will always receive all the  messages retained in the topic from the start.

\subsubsection{Consumer}
\hspace{2 mm} The consumer, implemented in Java, receives the messages sent by the producers. It connects to the Kafka broker with the function "subscribe" to a certain topic. While it is connected to the Kafka broker, it processes each record it gets from the broker with the class Utils. A record is in JSON format. This function implemented in the class Kafka will connect to a fixed broker with the topic "chat". It also setup output function for the pipeline.
\subsection{Flink}

\hspace{2 mm} To implement Flink, we need a source, which will send messages and a sink which will receive the messages. 

\subsubsection{Source}

\hspace{2 mm} For the source, we had a lot of troubles trying to implementing it in Python. We tried to implement a socket and an http server but it did not work well, so we decided to directly implement it in Java with  the function \textit{StreamExecutionEnvironnement.readTextFile} which creates a data stream from a file. Then we process messages by overriding the \textit{processElement} function from the \textit{Broadcast Process Function} class.

\subsubsection{Sink}

\hspace{2 mm} To create the sink, we need to create an instance of a SinkFunction where we override the processElement function. If we wanted to store the messages, we could have implement a second sink that stores messages.

\subsection{Kafka-Flink}

\hspace{2 mm} To implement Kafka-Flink, we decided to use Kafka as producer and Flink as consumer and message will be processed using Utils.
The advantage of this alliance is that Kafka already provides a broker.
The class Kafka-Flink will create a Kafka consumer source for Flink to use to receive message from producers.


\subsection{Shared library}
\hspace{2mm} We created a class Utils that contains functions used to process each message received by the chat and output it.

First we create an Hashset that is going to store all the banned users.

Then, the function \textit{processMessage}, implemented in the class Utils, process each message received:
\begin{itemize}
    \item Check if the user of the message is banned, if true then it will tell that the message of the user is blocked with the reason: "User Banned".
    \item Check if the message contains banned words, if true then it will block the message with the reason "Message contains banned word" and will ban the user of the message.
    \item If the message is not blocked then we will show the message on the chat.
\end{itemize}



\section{Benchmark}

\subsection{Setup}

\hspace{2 mm} To benchmark our application, we used the \href{https://flink.apache.org/2019/02/21/monitoring-apache-flink-applications-101/}{Flink monitoring}. We also used the \href{https://github.com/apache/kafka/blob/trunk/bin/kafka-producer-perf-test.sh}{Kafka Producer Benchmark} and the \href{https://github.com/apache/kafka/blob/trunk/bin/kafka-consumer-perf-test.sh}{Kafka Consumer Benchmark}

The tests were run on this machine:
\begin{itemize}
    \item Computer: Zenbook UM3402YAR\_UM3402YA 1.0
    \item CPU: AMD Ryzen 7 7730U with Radeon G
    \item Memory: 15383MiB
    \item OS: EndeavourOS Linux x86\_64
    \item Kernel: 6.12.1-zen1-1-zen
\end{itemize}

Each test was run 10 times and we took the average of it.


\subsection{Results}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.9\linewidth]{img/kafka_prod_bench.png}
    \caption{Benchmark of the ingestion rate of a topic}
    \label{fig:kafka-benchmark}
\end{figure}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.9\linewidth]{img/kafka_latency_quantiles.png}
    \caption{Kafka End-to-End Latency Quantiles}
    \label{fig:kafka_latency_quantiles}
\end{figure}

\begin{figure}[H]
    \centering
     \includegraphics[width=0.9\linewidth]{img/kafka_cons_bench.png}
     \caption{Benchmark from the topic to a consumer}
    \label{fig:kafka_cons_bench}
\end{figure}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.9\linewidth]{img/flink_bench.png}
    \caption{Flink Benchmark}
    \label{fig:flink-benchmark}
\end{figure}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.9\linewidth]{img/flink_processing.png}
    \caption{Kafka-Flink Consumer Benchmark}
    \label{fig:flink_processing}
\end{figure}



\subsection{Analysis of the results}
\hspace{2mm} On the Figure \ref{fig:kafka-benchmark}, we notice that the topic increases his ingestion speed as the number of messages increases. So the latency between sending a message and its retrieval and storage in the topic is consistently small.

Moreover we also notice on the Figure \ref{fig:kafka_cons_bench}, the topic increases his sending rate as the number of messages asked increase so sending 1 million of messages is as quick as sending only 1 thousand messages.

The Figure \ref{fig:kafka_latency_quantiles} shows that Kafka has a consistent and stable latency for larger record siez (1M+), with average latency remaining the same across all percentiles. However, it seems that smaller record sizes (e.g., 1K) show significant variability , especially at the 99th and 99.9th percentiles, where latency spikes due to overhead and inefficiencies in handling small batches. Optimal performance is achieved with moderate to large batch sizes, reducing network and processing overhead

The Figure \ref{fig:flink-benchmark} shows that the time taken to process messages is linear. Directly getting messages from the source without passing by a producer, 1 million and 6 hundreds thousands messages only take few seconds to be processed by the Flink.

On the Figure \ref{fig:flink_processing}, we notice that the growth for the time taken by the sending and processing of message by Kafka-Flink consumer is linear. Indeed, the growth between each factor of 10 for the number of messages varies from a factor of less than 2 to 8. However the time needed is much longer than Flink without producer, we have already proved that the topic and Flink are not the issue here hence the issue comes from our producer that sends messages to topic too slowly.

Having a single producer to send messages to the topic is not enough. The topic can ingest messages very quickly but a single producer does not sends quick enough. Thus to send lots of messages, we need to partition the messages between multiple producers to send messages to the topic.

\section{Conclusion}
\hspace{2 mm} As we seen in the figures, the main advantage of stream databases is the rapidity of execution. Kafka works well on its own and is consistent and have stable latency. Even with data as big as millions of records, it takes only a couple of seconds to be ingested by a topic (for Kafka) and being send to a consumer (For Kafka ). But we had a lot of issues implementing standalone sources with Flink. The main advantage of Flink is that it acts very well as processor, it can process a lot of messages very quickly. The combination of Kafka and Flink is in our opinion the implementation that works the best. It is easy to implement, and very well documented. The integration is commonly used and deeply developed. 


 \nocite{*}
\printbibliography
\end{document}
